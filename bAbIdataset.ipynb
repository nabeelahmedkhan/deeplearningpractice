{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bAbIdataset.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabeelahmedkhan/deeplearningpractice/blob/master/bAbIdataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQU_W1fxp_af",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmSyvdcZqDlS",
        "colab_type": "code",
        "outputId": "45134da1-a82b-41de-bcae-91655357945c",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2fb9185d-0d84-4b9f-91e6-bf5c20e86a91\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-2fb9185d-0d84-4b9f-91e6-bf5c20e86a91\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKswvkaeqSqn",
        "colab_type": "code",
        "outputId": "6e92e037-a150-49d2-aea4-f8634b56c729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''Trains a memory network on the bAbI dataset.\n",
        "References:\n",
        "- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n",
        "  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n",
        "  http://arxiv.org/abs/1502.05698\n",
        "- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n",
        "  \"End-To-End Memory Networks\",\n",
        "  http://arxiv.org/abs/1503.08895\n",
        "Reaches 98.6% accuracy on task 'single_supporting_fact_10k' after 120 epochs.\n",
        "Time per epoch: 3s on CPU (core i7).\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate\n",
        "from keras.layers import LSTM\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from functools import reduce\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "def tokenize(sent):\n",
        "    '''Return the tokens of a sentence including punctuation.\n",
        "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
        "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
        "    '''\n",
        "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
        "\n",
        "\n",
        "def parse_stories(lines, only_supporting=False):\n",
        "    '''Parse stories provided in the bAbi tasks format\n",
        "    If only_supporting is true, only the sentences\n",
        "    that support the answer are kept.\n",
        "    '''\n",
        "    data = []\n",
        "    story = []\n",
        "    for line in lines:\n",
        "        line = line.decode('utf-8').strip()\n",
        "        nid, line = line.split(' ', 1)\n",
        "        nid = int(nid)\n",
        "        if nid == 1:\n",
        "            story = []\n",
        "        if '\\t' in line:\n",
        "            q, a, supporting = line.split('\\t')\n",
        "            q = tokenize(q)\n",
        "            substory = None\n",
        "            if only_supporting:\n",
        "                # Only select the related substory\n",
        "                supporting = map(int, supporting.split())\n",
        "                substory = [story[i - 1] for i in supporting]\n",
        "            else:\n",
        "                # Provide all the substories\n",
        "                substory = [x for x in story if x]\n",
        "            data.append((substory, q, a))\n",
        "            story.append('')\n",
        "        else:\n",
        "            sent = tokenize(line)\n",
        "            story.append(sent)\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_stories(f, only_supporting=False, max_length=None):\n",
        "    '''Given a file name, read the file,\n",
        "    retrieve the stories,\n",
        "    and then convert the sentences into a single story.\n",
        "    If max_length is supplied,\n",
        "    any stories longer than max_length tokens will be discarded.\n",
        "    '''\n",
        "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
        "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
        "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
        "    return data\n",
        "\n",
        "\n",
        "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
        "    X = []\n",
        "    Xq = []\n",
        "    Y = []\n",
        "    for story, query, answer in data:\n",
        "        x = [word_idx[w] for w in story]\n",
        "        xq = [word_idx[w] for w in query]\n",
        "        # let's not forget that index 0 is reserved\n",
        "        y = np.zeros(len(word_idx) + 1)\n",
        "        y[word_idx[answer]] = 1\n",
        "        X.append(x)\n",
        "        Xq.append(xq)\n",
        "        Y.append(y)\n",
        "    return (pad_sequences(X, maxlen=story_maxlen),\n",
        "            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n",
        "\n",
        "try:\n",
        "    path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
        "except:\n",
        "    print('Error downloading dataset, please download it manually:\\n'\n",
        "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n",
        "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
        "    raise\n",
        "tar = tarfile.open(path)\n",
        "\n",
        "challenges = {\n",
        "    # QA1 with 10,000 samples\n",
        "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
        "    # QA2 with 10,000 samples\n",
        "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
        "}\n",
        "challenge_type = 'single_supporting_fact_10k'\n",
        "challenge = challenges[challenge_type]\n",
        "\n",
        "print('Extracting stories for the challenge:', challenge_type)\n",
        "train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
        "test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
        "\n",
        "vocab = set()\n",
        "for story, q, answer in train_stories + test_stories:\n",
        "    vocab |= set(story + q + [answer])\n",
        "vocab = sorted(vocab)\n",
        "\n",
        "# Reserve 0 for masking via pad_sequences\n",
        "vocab_size = len(vocab) + 1\n",
        "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
        "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
        "\n",
        "print('-')\n",
        "print('Vocab size:', vocab_size, 'unique words')\n",
        "print('Story max length:', story_maxlen, 'words')\n",
        "print('Query max length:', query_maxlen, 'words')\n",
        "print('Number of training stories:', len(train_stories))\n",
        "print('Number of test stories:', len(test_stories))\n",
        "print('-')\n",
        "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
        "print(train_stories[0])\n",
        "print('-')\n",
        "print('Vectorizing the word sequences...')\n",
        "\n",
        "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
        "inputs_train, queries_train, answers_train = vectorize_stories(train_stories,\n",
        "                                                               word_idx,\n",
        "                                                               story_maxlen,\n",
        "                                                               query_maxlen)\n",
        "inputs_test, queries_test, answers_test = vectorize_stories(test_stories,\n",
        "                                                            word_idx,\n",
        "                                                            story_maxlen,\n",
        "                                                            query_maxlen)\n",
        "\n",
        "print('-')\n",
        "print('inputs: integer tensor of shape (samples, max_length)')\n",
        "print('inputs_train shape:', inputs_train.shape)\n",
        "print('inputs_test shape:', inputs_test.shape)\n",
        "print('-')\n",
        "print('queries: integer tensor of shape (samples, max_length)')\n",
        "print('queries_train shape:', queries_train.shape)\n",
        "print('queries_test shape:', queries_test.shape)\n",
        "print('-')\n",
        "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
        "print('answers_train shape:', answers_train.shape)\n",
        "print('answers_test shape:', answers_test.shape)\n",
        "print('-')\n",
        "print('Compiling...')\n",
        "\n",
        "# placeholders\n",
        "input_sequence = Input((story_maxlen,))\n",
        "question = Input((query_maxlen,))\n",
        "\n",
        "# encoders\n",
        "# embed the input sequence into a sequence of vectors\n",
        "input_encoder_m = Sequential()\n",
        "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
        "                              output_dim=64))\n",
        "input_encoder_m.add(Dropout(0.3))\n",
        "# output: (samples, story_maxlen, embedding_dim)\n",
        "\n",
        "# embed the input into a sequence of vectors of size query_maxlen\n",
        "input_encoder_c = Sequential()\n",
        "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
        "                              output_dim=query_maxlen))\n",
        "input_encoder_c.add(Dropout(0.3))\n",
        "# output: (samples, story_maxlen, query_maxlen)\n",
        "\n",
        "# embed the question into a sequence of vectors\n",
        "question_encoder = Sequential()\n",
        "question_encoder.add(Embedding(input_dim=vocab_size,\n",
        "                               output_dim=64,\n",
        "                               input_length=query_maxlen))\n",
        "question_encoder.add(Dropout(0.3))\n",
        "# output: (samples, query_maxlen, embedding_dim)\n",
        "\n",
        "# encode input sequence and questions (which are indices)\n",
        "# to sequences of dense vectors\n",
        "input_encoded_m = input_encoder_m(input_sequence)\n",
        "input_encoded_c = input_encoder_c(input_sequence)\n",
        "question_encoded = question_encoder(question)\n",
        "\n",
        "# compute a 'match' between the first input vector sequence\n",
        "# and the question vector sequence\n",
        "# shape: `(samples, story_maxlen, query_maxlen)`\n",
        "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
        "match = Activation('softmax')(match)\n",
        "\n",
        "# add the match matrix with the second input vector sequence\n",
        "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
        "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
        "\n",
        "# concatenate the match matrix with the question vector sequence\n",
        "answer = concatenate([response, question_encoded])\n",
        "\n",
        "# the original paper uses a matrix multiplication for this reduction step.\n",
        "# we choose to use a RNN instead.\n",
        "answer = LSTM(32)(answer)  # (samples, 32)\n",
        "\n",
        "# one regularization layer -- more would probably be needed.\n",
        "answer = Dropout(0.3)(answer)\n",
        "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
        "# we output a probability distribution over the vocabulary\n",
        "answer = Activation('softmax')(answer)\n",
        "\n",
        "# build the final model\n",
        "model = Model([input_sequence, question], answer)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# train\n",
        "model.fit([inputs_train, queries_train], answers_train,\n",
        "          batch_size=32,\n",
        "          epochs=120,\n",
        "          validation_data=([inputs_test, queries_test], answers_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\n",
            "11747328/11745123 [==============================] - 1s 0us/step\n",
            "Extracting stories for the challenge: single_supporting_fact_10k\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
            "  return _compile(pattern, flags).split(string, maxsplit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Vocab size: 22 unique words\n",
            "Story max length: 68 words\n",
            "Query max length: 4 words\n",
            "Number of training stories: 10000\n",
            "Number of test stories: 1000\n",
            "-\n",
            "Here's what a \"story\" tuple looks like (input, query, answer):\n",
            "(['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'John', 'went', 'to', 'the', 'hallway', '.'], ['Where', 'is', 'Mary', '?'], 'bathroom')\n",
            "-\n",
            "Vectorizing the word sequences...\n",
            "-\n",
            "inputs: integer tensor of shape (samples, max_length)\n",
            "inputs_train shape: (10000, 68)\n",
            "inputs_test shape: (1000, 68)\n",
            "-\n",
            "queries: integer tensor of shape (samples, max_length)\n",
            "queries_train shape: (10000, 4)\n",
            "queries_test shape: (1000, 4)\n",
            "-\n",
            "answers: binary (1 or 0) tensor of shape (samples, vocab_size)\n",
            "answers_train shape: (10000, 22)\n",
            "answers_test shape: (1000, 22)\n",
            "-\n",
            "Compiling...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/120\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "10000/10000 [==============================] - 5s 524us/step - loss: 1.9813 - acc: 0.1685 - val_loss: 1.7957 - val_acc: 0.1590\n",
            "Epoch 2/120\n",
            "10000/10000 [==============================] - 4s 357us/step - loss: 1.7408 - acc: 0.2299 - val_loss: 1.6598 - val_acc: 0.2440\n",
            "Epoch 3/120\n",
            "10000/10000 [==============================] - 4s 354us/step - loss: 1.6469 - acc: 0.2965 - val_loss: 1.5934 - val_acc: 0.2940\n",
            "Epoch 4/120\n",
            "10000/10000 [==============================] - 4s 359us/step - loss: 1.5512 - acc: 0.3683 - val_loss: 1.4993 - val_acc: 0.3900\n",
            "Epoch 5/120\n",
            "10000/10000 [==============================] - 4s 373us/step - loss: 1.5125 - acc: 0.3904 - val_loss: 1.4806 - val_acc: 0.3970\n",
            "Epoch 6/120\n",
            "10000/10000 [==============================] - 4s 362us/step - loss: 1.4839 - acc: 0.4088 - val_loss: 1.4531 - val_acc: 0.4070\n",
            "Epoch 7/120\n",
            "10000/10000 [==============================] - 4s 357us/step - loss: 1.4620 - acc: 0.4306 - val_loss: 1.4271 - val_acc: 0.4280\n",
            "Epoch 8/120\n",
            "10000/10000 [==============================] - 4s 365us/step - loss: 1.4296 - acc: 0.4503 - val_loss: 1.3672 - val_acc: 0.4700\n",
            "Epoch 9/120\n",
            "10000/10000 [==============================] - 4s 354us/step - loss: 1.3965 - acc: 0.4629 - val_loss: 1.3491 - val_acc: 0.4750\n",
            "Epoch 10/120\n",
            "10000/10000 [==============================] - 4s 355us/step - loss: 1.3767 - acc: 0.4713 - val_loss: 1.3374 - val_acc: 0.4840\n",
            "Epoch 11/120\n",
            "10000/10000 [==============================] - 4s 356us/step - loss: 1.3596 - acc: 0.4800 - val_loss: 1.3267 - val_acc: 0.4990\n",
            "Epoch 12/120\n",
            "10000/10000 [==============================] - 4s 355us/step - loss: 1.3479 - acc: 0.4912 - val_loss: 1.3073 - val_acc: 0.4940\n",
            "Epoch 13/120\n",
            "10000/10000 [==============================] - 3s 348us/step - loss: 1.3227 - acc: 0.4945 - val_loss: 1.2809 - val_acc: 0.5250\n",
            "Epoch 14/120\n",
            "10000/10000 [==============================] - 4s 354us/step - loss: 1.3204 - acc: 0.5001 - val_loss: 1.2712 - val_acc: 0.5270\n",
            "Epoch 15/120\n",
            "10000/10000 [==============================] - 4s 357us/step - loss: 1.3099 - acc: 0.4984 - val_loss: 1.2852 - val_acc: 0.5260\n",
            "Epoch 16/120\n",
            "10000/10000 [==============================] - 4s 358us/step - loss: 1.2989 - acc: 0.5030 - val_loss: 1.2557 - val_acc: 0.5120\n",
            "Epoch 17/120\n",
            "10000/10000 [==============================] - 4s 361us/step - loss: 1.2774 - acc: 0.5095 - val_loss: 1.2405 - val_acc: 0.5240\n",
            "Epoch 18/120\n",
            "10000/10000 [==============================] - 4s 360us/step - loss: 1.2636 - acc: 0.5120 - val_loss: 1.2638 - val_acc: 0.5270\n",
            "Epoch 19/120\n",
            "10000/10000 [==============================] - 4s 361us/step - loss: 1.2476 - acc: 0.5166 - val_loss: 1.2080 - val_acc: 0.5250\n",
            "Epoch 20/120\n",
            "10000/10000 [==============================] - 4s 374us/step - loss: 1.2372 - acc: 0.5161 - val_loss: 1.2066 - val_acc: 0.5220\n",
            "Epoch 21/120\n",
            "10000/10000 [==============================] - 4s 359us/step - loss: 1.2212 - acc: 0.5171 - val_loss: 1.1982 - val_acc: 0.5110\n",
            "Epoch 22/120\n",
            "10000/10000 [==============================] - 4s 355us/step - loss: 1.2134 - acc: 0.5185 - val_loss: 1.1960 - val_acc: 0.5290\n",
            "Epoch 23/120\n",
            "10000/10000 [==============================] - 4s 351us/step - loss: 1.1989 - acc: 0.5217 - val_loss: 1.1908 - val_acc: 0.5140\n",
            "Epoch 24/120\n",
            "10000/10000 [==============================] - 4s 362us/step - loss: 1.1900 - acc: 0.5258 - val_loss: 1.2175 - val_acc: 0.5110\n",
            "Epoch 25/120\n",
            "10000/10000 [==============================] - 4s 355us/step - loss: 1.1835 - acc: 0.5205 - val_loss: 1.1871 - val_acc: 0.5160\n",
            "Epoch 26/120\n",
            "10000/10000 [==============================] - 4s 354us/step - loss: 1.1823 - acc: 0.5207 - val_loss: 1.1866 - val_acc: 0.5110\n",
            "Epoch 27/120\n",
            "10000/10000 [==============================] - 4s 359us/step - loss: 1.1682 - acc: 0.5258 - val_loss: 1.1736 - val_acc: 0.5100\n",
            "Epoch 28/120\n",
            "10000/10000 [==============================] - 3s 349us/step - loss: 1.1738 - acc: 0.5270 - val_loss: 1.1720 - val_acc: 0.5140\n",
            "Epoch 29/120\n",
            "10000/10000 [==============================] - 4s 364us/step - loss: 1.1659 - acc: 0.5255 - val_loss: 1.1804 - val_acc: 0.5000\n",
            "Epoch 30/120\n",
            "10000/10000 [==============================] - 4s 354us/step - loss: 1.1574 - acc: 0.5313 - val_loss: 1.1841 - val_acc: 0.4960\n",
            "Epoch 31/120\n",
            "10000/10000 [==============================] - 4s 366us/step - loss: 1.1491 - acc: 0.5295 - val_loss: 1.1695 - val_acc: 0.5140\n",
            "Epoch 32/120\n",
            "10000/10000 [==============================] - 3s 348us/step - loss: 1.1466 - acc: 0.5332 - val_loss: 1.1710 - val_acc: 0.5220\n",
            "Epoch 33/120\n",
            "10000/10000 [==============================] - 3s 349us/step - loss: 1.1288 - acc: 0.5419 - val_loss: 1.1796 - val_acc: 0.5060\n",
            "Epoch 34/120\n",
            "10000/10000 [==============================] - 4s 361us/step - loss: 1.1345 - acc: 0.5402 - val_loss: 1.1692 - val_acc: 0.5190\n",
            "Epoch 35/120\n",
            "10000/10000 [==============================] - 4s 361us/step - loss: 1.1150 - acc: 0.5533 - val_loss: 1.1658 - val_acc: 0.5280\n",
            "Epoch 36/120\n",
            "10000/10000 [==============================] - 4s 358us/step - loss: 1.1080 - acc: 0.5569 - val_loss: 1.1355 - val_acc: 0.5530\n",
            "Epoch 37/120\n",
            "10000/10000 [==============================] - 4s 354us/step - loss: 1.0798 - acc: 0.5703 - val_loss: 1.1054 - val_acc: 0.5830\n",
            "Epoch 38/120\n",
            "10000/10000 [==============================] - 4s 353us/step - loss: 1.0368 - acc: 0.6004 - val_loss: 1.0308 - val_acc: 0.6110\n",
            "Epoch 39/120\n",
            "10000/10000 [==============================] - 4s 359us/step - loss: 0.9647 - acc: 0.6410 - val_loss: 0.9149 - val_acc: 0.6710\n",
            "Epoch 40/120\n",
            "10000/10000 [==============================] - 4s 355us/step - loss: 0.8710 - acc: 0.6890 - val_loss: 0.8009 - val_acc: 0.7220\n",
            "Epoch 41/120\n",
            "10000/10000 [==============================] - 4s 359us/step - loss: 0.7679 - acc: 0.7359 - val_loss: 0.7296 - val_acc: 0.7500\n",
            "Epoch 42/120\n",
            "10000/10000 [==============================] - 3s 350us/step - loss: 0.7062 - acc: 0.7525 - val_loss: 0.6902 - val_acc: 0.7520\n",
            "Epoch 43/120\n",
            "10000/10000 [==============================] - 4s 362us/step - loss: 0.6545 - acc: 0.7718 - val_loss: 0.6704 - val_acc: 0.7420\n",
            "Epoch 44/120\n",
            "10000/10000 [==============================] - 4s 372us/step - loss: 0.6126 - acc: 0.7837 - val_loss: 0.6425 - val_acc: 0.7620\n",
            "Epoch 45/120\n",
            "10000/10000 [==============================] - 4s 354us/step - loss: 0.5935 - acc: 0.7867 - val_loss: 0.6044 - val_acc: 0.7670\n",
            "Epoch 46/120\n",
            "10000/10000 [==============================] - 4s 363us/step - loss: 0.5552 - acc: 0.8011 - val_loss: 0.5662 - val_acc: 0.7940\n",
            "Epoch 47/120\n",
            "10000/10000 [==============================] - 3s 349us/step - loss: 0.5223 - acc: 0.8110 - val_loss: 0.5844 - val_acc: 0.7740\n",
            "Epoch 48/120\n",
            "10000/10000 [==============================] - 4s 350us/step - loss: 0.4823 - acc: 0.8223 - val_loss: 0.5144 - val_acc: 0.8000\n",
            "Epoch 49/120\n",
            "10000/10000 [==============================] - 3s 345us/step - loss: 0.4480 - acc: 0.8354 - val_loss: 0.4701 - val_acc: 0.8220\n",
            "Epoch 50/120\n",
            "10000/10000 [==============================] - 4s 359us/step - loss: 0.4263 - acc: 0.8447 - val_loss: 0.4553 - val_acc: 0.8310\n",
            "Epoch 51/120\n",
            "10000/10000 [==============================] - 4s 355us/step - loss: 0.3981 - acc: 0.8510 - val_loss: 0.4126 - val_acc: 0.8400\n",
            "Epoch 52/120\n",
            "10000/10000 [==============================] - 4s 356us/step - loss: 0.3873 - acc: 0.8608 - val_loss: 0.3982 - val_acc: 0.8430\n",
            "Epoch 53/120\n",
            "10000/10000 [==============================] - 4s 367us/step - loss: 0.3658 - acc: 0.8634 - val_loss: 0.3899 - val_acc: 0.8470\n",
            "Epoch 54/120\n",
            "10000/10000 [==============================] - 4s 367us/step - loss: 0.3574 - acc: 0.8680 - val_loss: 0.3734 - val_acc: 0.8550\n",
            "Epoch 55/120\n",
            "10000/10000 [==============================] - 4s 375us/step - loss: 0.3466 - acc: 0.8739 - val_loss: 0.3629 - val_acc: 0.8610\n",
            "Epoch 56/120\n",
            "10000/10000 [==============================] - 4s 381us/step - loss: 0.3307 - acc: 0.8807 - val_loss: 0.3649 - val_acc: 0.8600\n",
            "Epoch 57/120\n",
            "10000/10000 [==============================] - 4s 365us/step - loss: 0.3247 - acc: 0.8807 - val_loss: 0.3671 - val_acc: 0.8510\n",
            "Epoch 58/120\n",
            "10000/10000 [==============================] - 4s 359us/step - loss: 0.3018 - acc: 0.8862 - val_loss: 0.3838 - val_acc: 0.8500\n",
            "Epoch 59/120\n",
            "10000/10000 [==============================] - 4s 365us/step - loss: 0.3068 - acc: 0.8851 - val_loss: 0.3452 - val_acc: 0.8550\n",
            "Epoch 60/120\n",
            "10000/10000 [==============================] - 4s 358us/step - loss: 0.2939 - acc: 0.8892 - val_loss: 0.3328 - val_acc: 0.8720\n",
            "Epoch 61/120\n",
            "10000/10000 [==============================] - 4s 361us/step - loss: 0.2859 - acc: 0.8955 - val_loss: 0.3229 - val_acc: 0.8710\n",
            "Epoch 62/120\n",
            "10000/10000 [==============================] - 4s 366us/step - loss: 0.2838 - acc: 0.8910 - val_loss: 0.3194 - val_acc: 0.8740\n",
            "Epoch 63/120\n",
            "10000/10000 [==============================] - 4s 361us/step - loss: 0.2709 - acc: 0.8985 - val_loss: 0.3256 - val_acc: 0.8700\n",
            "Epoch 64/120\n",
            "10000/10000 [==============================] - 4s 364us/step - loss: 0.2632 - acc: 0.9047 - val_loss: 0.3031 - val_acc: 0.8910\n",
            "Epoch 65/120\n",
            "10000/10000 [==============================] - 4s 364us/step - loss: 0.2565 - acc: 0.9055 - val_loss: 0.2994 - val_acc: 0.8780\n",
            "Epoch 66/120\n",
            "10000/10000 [==============================] - 4s 351us/step - loss: 0.2432 - acc: 0.9116 - val_loss: 0.3354 - val_acc: 0.8770\n",
            "Epoch 67/120\n",
            "10000/10000 [==============================] - 4s 365us/step - loss: 0.2439 - acc: 0.9084 - val_loss: 0.2818 - val_acc: 0.8850\n",
            "Epoch 68/120\n",
            "10000/10000 [==============================] - 4s 367us/step - loss: 0.2310 - acc: 0.9134 - val_loss: 0.2763 - val_acc: 0.8830\n",
            "Epoch 69/120\n",
            "10000/10000 [==============================] - 4s 354us/step - loss: 0.2277 - acc: 0.9163 - val_loss: 0.2591 - val_acc: 0.8980\n",
            "Epoch 70/120\n",
            "10000/10000 [==============================] - 4s 358us/step - loss: 0.2208 - acc: 0.9189 - val_loss: 0.2486 - val_acc: 0.8960\n",
            "Epoch 71/120\n",
            "10000/10000 [==============================] - 4s 363us/step - loss: 0.2208 - acc: 0.9202 - val_loss: 0.2908 - val_acc: 0.8810\n",
            "Epoch 72/120\n",
            "10000/10000 [==============================] - 4s 360us/step - loss: 0.2095 - acc: 0.9220 - val_loss: 0.2547 - val_acc: 0.9050\n",
            "Epoch 73/120\n",
            "10000/10000 [==============================] - 4s 368us/step - loss: 0.1957 - acc: 0.9289 - val_loss: 0.2614 - val_acc: 0.8940\n",
            "Epoch 74/120\n",
            "10000/10000 [==============================] - 4s 366us/step - loss: 0.1938 - acc: 0.9266 - val_loss: 0.2439 - val_acc: 0.9090\n",
            "Epoch 75/120\n",
            "10000/10000 [==============================] - 4s 361us/step - loss: 0.1849 - acc: 0.9325 - val_loss: 0.2252 - val_acc: 0.9100\n",
            "Epoch 76/120\n",
            "10000/10000 [==============================] - 4s 365us/step - loss: 0.1847 - acc: 0.9306 - val_loss: 0.2412 - val_acc: 0.9080\n",
            "Epoch 77/120\n",
            "10000/10000 [==============================] - 4s 363us/step - loss: 0.1749 - acc: 0.9362 - val_loss: 0.2318 - val_acc: 0.9120\n",
            "Epoch 78/120\n",
            "10000/10000 [==============================] - 4s 361us/step - loss: 0.1728 - acc: 0.9387 - val_loss: 0.2151 - val_acc: 0.9180\n",
            "Epoch 79/120\n",
            "10000/10000 [==============================] - 4s 364us/step - loss: 0.1741 - acc: 0.9369 - val_loss: 0.2119 - val_acc: 0.9210\n",
            "Epoch 80/120\n",
            "10000/10000 [==============================] - 4s 360us/step - loss: 0.1653 - acc: 0.9413 - val_loss: 0.2179 - val_acc: 0.9210\n",
            "Epoch 81/120\n",
            "10000/10000 [==============================] - 4s 365us/step - loss: 0.1652 - acc: 0.9406 - val_loss: 0.2074 - val_acc: 0.9200\n",
            "Epoch 82/120\n",
            "10000/10000 [==============================] - 4s 365us/step - loss: 0.1575 - acc: 0.9412 - val_loss: 0.1953 - val_acc: 0.9320\n",
            "Epoch 83/120\n",
            "10000/10000 [==============================] - 4s 360us/step - loss: 0.1520 - acc: 0.9407 - val_loss: 0.1923 - val_acc: 0.9310\n",
            "Epoch 84/120\n",
            "10000/10000 [==============================] - 4s 354us/step - loss: 0.1405 - acc: 0.9496 - val_loss: 0.1953 - val_acc: 0.9280\n",
            "Epoch 85/120\n",
            "10000/10000 [==============================] - 4s 356us/step - loss: 0.1435 - acc: 0.9484 - val_loss: 0.1918 - val_acc: 0.9310\n",
            "Epoch 86/120\n",
            "10000/10000 [==============================] - 4s 356us/step - loss: 0.1434 - acc: 0.9476 - val_loss: 0.2253 - val_acc: 0.9230\n",
            "Epoch 87/120\n",
            "10000/10000 [==============================] - 4s 354us/step - loss: 0.1342 - acc: 0.9517 - val_loss: 0.2148 - val_acc: 0.9260\n",
            "Epoch 88/120\n",
            "10000/10000 [==============================] - 4s 361us/step - loss: 0.1335 - acc: 0.9511 - val_loss: 0.1856 - val_acc: 0.9330\n",
            "Epoch 89/120\n",
            "10000/10000 [==============================] - 4s 362us/step - loss: 0.1337 - acc: 0.9503 - val_loss: 0.1826 - val_acc: 0.9350\n",
            "Epoch 90/120\n",
            "10000/10000 [==============================] - 4s 361us/step - loss: 0.1243 - acc: 0.9550 - val_loss: 0.1787 - val_acc: 0.9430\n",
            "Epoch 91/120\n",
            "10000/10000 [==============================] - 4s 360us/step - loss: 0.1266 - acc: 0.9525 - val_loss: 0.1736 - val_acc: 0.9390\n",
            "Epoch 92/120\n",
            "10000/10000 [==============================] - 4s 362us/step - loss: 0.1240 - acc: 0.9570 - val_loss: 0.1723 - val_acc: 0.9470\n",
            "Epoch 93/120\n",
            "10000/10000 [==============================] - 4s 367us/step - loss: 0.1196 - acc: 0.9556 - val_loss: 0.1632 - val_acc: 0.9450\n",
            "Epoch 94/120\n",
            "10000/10000 [==============================] - 4s 363us/step - loss: 0.1138 - acc: 0.9592 - val_loss: 0.1941 - val_acc: 0.9400\n",
            "Epoch 95/120\n",
            "10000/10000 [==============================] - 4s 360us/step - loss: 0.1121 - acc: 0.9585 - val_loss: 0.1961 - val_acc: 0.9390\n",
            "Epoch 96/120\n",
            "10000/10000 [==============================] - 4s 356us/step - loss: 0.1108 - acc: 0.9579 - val_loss: 0.1701 - val_acc: 0.9450\n",
            "Epoch 97/120\n",
            "10000/10000 [==============================] - 4s 363us/step - loss: 0.1107 - acc: 0.9616 - val_loss: 0.1853 - val_acc: 0.9440\n",
            "Epoch 98/120\n",
            "10000/10000 [==============================] - 4s 359us/step - loss: 0.1036 - acc: 0.9625 - val_loss: 0.1874 - val_acc: 0.9320\n",
            "Epoch 99/120\n",
            "10000/10000 [==============================] - 4s 361us/step - loss: 0.1052 - acc: 0.9643 - val_loss: 0.1718 - val_acc: 0.9410\n",
            "Epoch 100/120\n",
            "10000/10000 [==============================] - 4s 362us/step - loss: 0.1040 - acc: 0.9635 - val_loss: 0.1846 - val_acc: 0.9380\n",
            "Epoch 101/120\n",
            "10000/10000 [==============================] - 4s 357us/step - loss: 0.0989 - acc: 0.9647 - val_loss: 0.1610 - val_acc: 0.9510\n",
            "Epoch 102/120\n",
            "10000/10000 [==============================] - 4s 355us/step - loss: 0.0942 - acc: 0.9667 - val_loss: 0.1917 - val_acc: 0.9460\n",
            "Epoch 103/120\n",
            "10000/10000 [==============================] - 4s 364us/step - loss: 0.0939 - acc: 0.9683 - val_loss: 0.1708 - val_acc: 0.9490\n",
            "Epoch 104/120\n",
            "10000/10000 [==============================] - 4s 358us/step - loss: 0.1015 - acc: 0.9658 - val_loss: 0.1540 - val_acc: 0.9520\n",
            "Epoch 105/120\n",
            "10000/10000 [==============================] - 4s 372us/step - loss: 0.0946 - acc: 0.9669 - val_loss: 0.1717 - val_acc: 0.9490\n",
            "Epoch 106/120\n",
            "10000/10000 [==============================] - 4s 360us/step - loss: 0.0898 - acc: 0.9695 - val_loss: 0.1615 - val_acc: 0.9490\n",
            "Epoch 107/120\n",
            "10000/10000 [==============================] - 4s 360us/step - loss: 0.0860 - acc: 0.9709 - val_loss: 0.1574 - val_acc: 0.9480\n",
            "Epoch 108/120\n",
            "10000/10000 [==============================] - 4s 358us/step - loss: 0.0920 - acc: 0.9666 - val_loss: 0.1700 - val_acc: 0.9480\n",
            "Epoch 109/120\n",
            "10000/10000 [==============================] - 4s 366us/step - loss: 0.0916 - acc: 0.9690 - val_loss: 0.1784 - val_acc: 0.9480\n",
            "Epoch 110/120\n",
            "10000/10000 [==============================] - 4s 354us/step - loss: 0.0857 - acc: 0.9680 - val_loss: 0.1719 - val_acc: 0.9450\n",
            "Epoch 111/120\n",
            "10000/10000 [==============================] - 4s 351us/step - loss: 0.0832 - acc: 0.9708 - val_loss: 0.1468 - val_acc: 0.9530\n",
            "Epoch 112/120\n",
            "10000/10000 [==============================] - 4s 360us/step - loss: 0.0830 - acc: 0.9717 - val_loss: 0.1531 - val_acc: 0.9510\n",
            "Epoch 113/120\n",
            "10000/10000 [==============================] - 4s 355us/step - loss: 0.0853 - acc: 0.9711 - val_loss: 0.1510 - val_acc: 0.9530\n",
            "Epoch 114/120\n",
            "10000/10000 [==============================] - 4s 364us/step - loss: 0.0836 - acc: 0.9718 - val_loss: 0.1615 - val_acc: 0.9490\n",
            "Epoch 115/120\n",
            "10000/10000 [==============================] - 4s 370us/step - loss: 0.0850 - acc: 0.9732 - val_loss: 0.1609 - val_acc: 0.9500\n",
            "Epoch 116/120\n",
            "10000/10000 [==============================] - 4s 358us/step - loss: 0.0776 - acc: 0.9744 - val_loss: 0.1587 - val_acc: 0.9490\n",
            "Epoch 117/120\n",
            "10000/10000 [==============================] - 4s 361us/step - loss: 0.0788 - acc: 0.9744 - val_loss: 0.1720 - val_acc: 0.9460\n",
            "Epoch 118/120\n",
            "10000/10000 [==============================] - 4s 360us/step - loss: 0.0791 - acc: 0.9744 - val_loss: 0.1421 - val_acc: 0.9560\n",
            "Epoch 119/120\n",
            "10000/10000 [==============================] - 4s 353us/step - loss: 0.0693 - acc: 0.9760 - val_loss: 0.1748 - val_acc: 0.9420\n",
            "Epoch 120/120\n",
            "10000/10000 [==============================] - 4s 352us/step - loss: 0.0693 - acc: 0.9767 - val_loss: 0.1678 - val_acc: 0.9490\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f00a05ffe48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpl_joutqe7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}